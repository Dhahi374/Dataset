--- SPARK CLUSTER FOR RASPBERRY PI ---

1. Choose a Master machine (any machine you want)
In the Master, execute these commands :


1.1. wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.6.tgz
1.2. sudo tar -zxvf spark-1.4.0-bin-hadoop2.6.tgz -C /opt/
1.3. sudo ln -s /opt/spark-1.4.0-bin-hadoop2.6 /opt/spark
1.4. Take the spark-env.sh (located in /opt/spark/conf/) from the master machine (.105) and copy it in other slaves in /opt/spark/conf/spark-env.sh
1.5. ssh-keygen -b 2048 -t rsa
1.6. ssh-copy-id user@ip (user and ip of the slaves)
1.7. /opt/spark/sbin/start-master.sh (The master starts)


sudo scp /opt/spark/file1.txt pi@192.168.**:
sudo scp /opt/spark/conf/spark-env.sh pi@192.**:
nano /etc/dphys-swapfile
CONF_SWAPSIZE=1024


2. For slave machines:

2.1. wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.6.tgz
2.2. sudo tar -zxvf spark-1.4.0-bin-hadoop2.6.tgz -C /opt/
2.3. sudo ln -s /opt/spark-1.4.0-bin-hadoop2.6 /opt/spark
2.4. Take the spark-env.sh (located in /opt/spark/conf/) from the master machine (.105) and copy it in other slaves in /opt/spark/conf/spark-env.sh
2.5. ssh-keygen -b 2048 -t rsa
2.6. ssh-copy-id user@ip (user and ip of the master)
2.7. sudo /opt/spark/sbin/start-slave.sh ip:7077/ (ip of the master)

( Edit /etc/hosts << localhost Dh10)



3. For benchmarking (You can use any machine (pc, raspberry..)

3.1. wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.6.tgz
3.2. sudo tar -zxvf spark-1.4.0-bin-hadoop2.6.tgz -C /opt/
3.3. sudo ln -s /opt/spark-1.4.0-bin-hadoop2.6 /opt/spark
3.4. sudo /opt/spark/bin/spark-shell --master spark://ip:7077 (ip of the master)
3.5. Once the shell finishes initialisation, type in the command line of Scala:

val changeFile = sc.textFile("/opt/spark/CHANGES.txt")

val changeFileLower = changeFile.map(_.toLowerCase)

val changeFlatMap = changeFileLower.flatMap("[a-z]+".r findAllIn _)

val changeMR = changeFlatMap.map(word => (word,1)).reduceByKey(_ + _)

changeMR.take(10)

System.exit(1)
 






sudo perf_3.18 stat -B /opt/spark/bin/spark-shell --master spark://192.168.1.105:7077 -i TestScala.scala 2>&1 | grep "insns per cycle" | awk '{print $1;}'



second file size : 6488667
third file size : 17893224


val count = sc.parallelize(1 to 100000).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()
println(s"Pi is roughly ${4.0 * count / 100000}")




incoming: 22416 Bytes
  - outgoing: 18237 Bytes
incoming: 22416 Bytes
  - outgoing: 17559 Bytes
incoming: 22312 Bytes
  - outgoing: 17559 Bytes
incoming: 22460 Bytes
  - outgoing: 17866 Bytes
incoming: 22380 Bytes
  - outgoing: 17566 Bytes



